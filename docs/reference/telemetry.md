# Telemetry – Metrics, Units, and File Formats

Defines what the harness records per window and how it is written to disk.
Use this spec to interpret CSV/JSON and build plots consistently.

## Files
- `results/<run-name>/kernel-data/<kernel>/telemetry.ndjson` - Per-window telemetry data (NDJSON format, default)
- `results/<run-name>/kernel-data/<kernel>/telemetry.csv` - Per-window telemetry data (CSV format, alternative)
- `results/<run-name>/kernel-data/<kernel>/report.html` - Interactive HTML report with visualizations (auto-generated)
- `results/<run-name>/analysis/` - Analysis plots and summary (generated by `cortex analyze`)

Format is determined by the `output.format` configuration setting (`ndjson` or `csv`, defaults to `ndjson`).

## Implementation Status

**Currently Collected** (Fall 2025):
- ✅ Timing: `release_ts_ns`, `deadline_ts_ns`, `start_ts_ns`, `end_ts_ns`
- ✅ Deadline tracking: `deadline_missed` flag
- ✅ Context: `W`, `H`, `C`, `Fs`, `warmup`, `repeat`, `run_id`, `plugin` name
- ✅ Derived metrics: latency, jitter (p50/p95/p99), throughput (computed in analysis)

**Deferred to Spring 2026**:
- ⚠️ Energy: `energy_j`, `power_mw` (RAPL integration planned for Linux x86)
- ⚠️ Memory: `rss_bytes` (runtime RSS measurement not implemented)
- ⚠️ Runtime memory: `state_bytes`, `workspace_bytes` (currently from plugin metadata only, not runtime measurement)

**Note**: Energy and memory fields may appear in the schema but are currently zero/unset in actual telemetry output. See [Future Enhancements](../development/future-enhancements.md) for implementation timeline.

## Common columns (per window)
| Column | Unit | Notes |
|---|---|---|
| run_id | — | Hash or label of the run |
| plugin | — | Kernel name (`car`, `notch_iir`, …) |
| dtype | — | `float32` \| `q15` \| `q7` |
| window_index | — | 0-based |
| release_ts_ns | ns | Monotonic clock |
| deadline_ts_ns | ns | Monotonic clock |
| start_ts_ns | ns | When `process()` called |
| end_ts_ns | ns | When output ready |
| deadline_missed | 0/1 | `end_ts_ns > deadline_ts_ns` |

## Device timing fields (per window)

**Status**: Available in v0.4.0+ when using device adapters

| Column | Unit | Notes |
|---|---|---|
| adapter_name | — | Name of device adapter used (e.g., `x86@loopback`, `jetson@tcp`) |
| device_tin_ns | ns | Time adapter received window from harness |
| device_tstart_ns | ns | Time adapter started kernel execution |
| device_tend_ns | ns | Time adapter finished kernel execution |
| device_tfirst_tx_ns | ns | Time adapter sent first response chunk |
| device_tlast_tx_ns | ns | Time adapter sent last response chunk |

**Notes**:
- Device timing fields enable measuring adapter overhead and network latency
- For local adapters (x86@loopback): device timing ≈ harness timing (socketpair overhead ~microseconds)
- For remote adapters (jetson@tcp, stm32@uart): device timing captures network/serial latency
- Adapter overhead = `(device_tin_ns - start_ts_ns) + (end_ts_ns - device_tlast_tx_ns)`

## Derived timing metrics (per window)
| Metric | Unit | Definition |
|---|---|---|
| latency_ns | ns | `end_ts_ns - last_input_arrival_ts_ns` |
| jitter_p95_minus_p50 | ns | computed per run |
| jitter_p99_minus_p50 | ns | computed per run |
| throughput_windows_per_s | 1/s | computed per run |

> Notes: jitter summaries (p50, p95, p99) are reported per plugin per run.

## Memory metrics (per window or per run)
| Metric | Unit | Notes |
|---|---|---|
| rss_bytes | bytes | **Not currently measured** (deferred to Spring 2026) |
| state_bytes | bytes | From plugin metadata (static, not runtime measurement) |
| workspace_bytes | bytes | From plugin metadata (static, not runtime measurement) |

**Note**: Memory metrics are currently limited. `state_bytes` and `workspace_bytes` come from plugin metadata (`cortex_get_info()`) and represent declared memory requirements, not actual runtime allocations. RSS measurement is planned for Spring 2026.

## Energy/Power (per window)
| Metric | Unit | Definition |
|---|---|---|
| energy_j | J | RAPL delta around `process()` - **Currently unused** (deferred to Spring 2026) |
| power_mw | mW | `energy_j * (Fs / H) * 1000` - **Currently unused** (deferred to Spring 2026) |

**Note**: Energy fields are defined in the telemetry schema but are **not currently populated**. Energy measurement via RAPL (Linux x86) is planned for Spring 2026. Current telemetry files will have these fields as zero or unset.

## Shape/context columns
| Column | Unit | Notes |
|---|---|---|
| W | samples | Window length |
| H | samples | Hop length |
| C | channels | Input channels |
| Fs | Hz | Sample rate |
| load_profile | — | `idle` \| `medium` \| `heavy` |
| repeat | — | Repeat index |
| warmup | 0/1 | Excluded from stats if 1 |

## Output Formats

### CSV Format
- Delimiter: `,`
- Header: yes (first line contains column names)
- Encoding: UTF-8
- File extension: `.csv`

### NDJSON Format

NDJSON (Newline-Delimited JSON) provides the same data as CSV in a streaming JSON format.

**Specification:** https://github.com/ndjson/ndjson-spec

**Format characteristics:**
- Each line contains ONE complete JSON object
- Lines separated by newline (`\n`)
- No outer array brackets
- Streamable and appendable
- File extension: `.ndjson`

**Configuration:**
```yaml
output:
  format: "ndjson"  # default (or "csv")
```

**Example output:**
```json
{"run_id":"1762310612183","plugin":"goertzel","window_index":0,"release_ts_ns":21194971498000,"deadline_ts_ns":21195471498000,"start_ts_ns":21194971498000,"end_ts_ns":21194971740000,"deadline_missed":0,"W":160,"H":80,"C":64,"Fs":160,"warmup":0,"repeat":1}
{"run_id":"1762310612183","plugin":"goertzel","window_index":1,"release_ts_ns":21195476495000,"deadline_ts_ns":21195976495000,"start_ts_ns":21195476495000,"end_ts_ns":21195476742000,"deadline_missed":0,"W":160,"H":80,"C":64,"Fs":160,"warmup":0,"repeat":1}
```

**Field mapping:**
All fields are identical to CSV columns (see "Common columns" above).

**Advantages:**
- Machine-readable structured format
- Streamable (no need to parse entire file)
- Compatible with tools like `jq`, Python `json.loads()`
- Can append records without rewriting file
- Standard format for log aggregation systems

**Usage with jq (optional tool):**

> **Note:** `jq` is not required and not installed by default. Install with:
> - macOS: `brew install jq`
> - Ubuntu/Debian: `sudo apt-get install jq`
> - Or use Python examples below (no additional dependencies)

```bash
# Pretty-print first record
head -1 results/run-*/kernel-data/*/telemetry.ndjson | jq .

# Extract all latencies
cat results/run-*/kernel-data/*/telemetry.ndjson | jq -r '.end_ts_ns - .start_ts_ns'

# Count deadline misses
cat results/run-*/kernel-data/*/telemetry.ndjson | jq -r '.deadline_missed' | grep 1 | wc -l
```

**Python alternatives (no extra dependencies):**
```bash
# Pretty-print first record
head -1 results/run-*/kernel-data/*/telemetry.ndjson | python -m json.tool

# Extract all latencies (Python one-liner)
python -c "
import json, glob
for path in glob.glob('results/run-*/kernel-data/*/telemetry.ndjson'):
    for line in open(path):
        data = json.loads(line)
        print(data['end_ts_ns'] - data['start_ts_ns'])
"

# Count deadline misses
python -c "
import json, glob
count = 0
for path in glob.glob('results/run-*/kernel-data/*/telemetry.ndjson'):
    for line in open(path):
        if json.loads(line).get('deadline_missed', 0) == 1:
            count += 1
print(count)
"
```

## Aggregates (per plugin per run)
- p50/p95/p99 of latency (ns)
- jitter: p95−p50, p99−p50 (ns)
- deadline_miss_rate (%)
- mean/median power (mW)
- summary table written to `results/<run-name>/analysis/SUMMARY.md`

## HTML Report

After all plugins complete execution, the harness automatically generates an interactive HTML report at `results/<run-name>/kernel-data/<kernel>/report.html` containing:

- **Summary table**: Key metrics (P50/P95/P99 latency, jitter, deadline miss rate) for all kernels
- **Per-kernel visualizations**:
  - Latency distribution histogram
  - Latency over time line plot
  - Deadline misses highlighted
- **Self-contained**: Embedded SVG plots, no external dependencies

The report is generated automatically after the plugin execution loop completes, providing a comprehensive overview of benchmark results with both summary statistics and detailed visualizations.

## Timing Accuracy

Latency measurements use `CLOCK_MONOTONIC` for high-resolution timing:

- **macOS**: 1µs resolution (quantization to microsecond increments)
- **Linux**: Typically 1ns resolution (nanosecond precision)
- **Measurement overhead**: < 100ns (negligible)

For your midterm report, note that:
- Measurements are accurate to ±1µs on macOS (acceptable for 6-8µs latencies)
- Relative comparisons (kernel A vs B) are valid despite quantization
- Statistical aggregates (P50/P95/P99) smooth out quantization effects

To verify timing accuracy, run: `make -C tests test-clock-resolution`
