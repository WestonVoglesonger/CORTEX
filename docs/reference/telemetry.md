# Telemetry – Metrics, Units, and File Formats

Defines what the harness records per window and how it is written to disk.
Use this spec to interpret CSV/JSON and build plots consistently.

## Files
- `results/<run-name>/kernel-data/<kernel>/telemetry.ndjson` - Per-window telemetry data (NDJSON format, default)
- `results/<run-name>/kernel-data/<kernel>/telemetry.csv` - Per-window telemetry data (CSV format, alternative)
- `results/<run-name>/kernel-data/<kernel>/report.html` - Interactive HTML report with visualizations (auto-generated)
- `results/<run-name>/analysis/` - Analysis plots and summary (generated by `cortex analyze`)

Format is determined by the `output.format` configuration setting (`ndjson` or `csv`, defaults to `ndjson`).

## Common columns (per window)
| Column | Unit | Notes |
|---|---|---|
| run_id | — | Hash or label of the run |
| plugin | — | Kernel name (`car`, `notch_iir`, …) |
| dtype | — | `float32` \| `q15` \| `q7` |
| window_index | — | 0-based |
| release_ts_ns | ns | Monotonic clock |
| deadline_ts_ns | ns | Monotonic clock |
| start_ts_ns | ns | When `process()` called |
| end_ts_ns | ns | When output ready |
| deadline_missed | 0/1 | `end_ts_ns > deadline_ts_ns` |

## Derived timing metrics (per window)
| Metric | Unit | Definition |
|---|---|---|
| latency_ns | ns | `end_ts_ns - last_input_arrival_ts_ns` |
| jitter_p95_minus_p50 | ns | computed per run |
| jitter_p99_minus_p50 | ns | computed per run |
| throughput_windows_per_s | 1/s | computed per run |

> Notes: jitter summaries (p50, p95, p99) are reported per plugin per run.

## Memory metrics (per window or per run)
| Metric | Unit | Notes |
|---|---|---|
| rss_bytes | bytes | Process RSS (approx) |
| state_bytes | bytes | From plugin metadata |
| workspace_bytes | bytes | From plugin metadata |

## Energy/Power (per window)
| Metric | Unit | Definition |
|---|---|---|
| energy_j | J | RAPL delta around `process()` |
| power_mw | mW | `energy_j * (Fs / H) * 1000` |

## Shape/context columns
| Column | Unit | Notes |
|---|---|---|
| W | samples | Window length |
| H | samples | Hop length |
| C | channels | Input channels |
| Fs | Hz | Sample rate |
| load_profile | — | `idle` \| `medium` \| `heavy` |
| repeat | — | Repeat index |
| warmup | 0/1 | Excluded from stats if 1 |

## Output Formats

### CSV Format
- Delimiter: `,`
- Header: yes (first line contains column names)
- Encoding: UTF-8
- File extension: `.csv`

### NDJSON Format

NDJSON (Newline-Delimited JSON) provides the same data as CSV in a streaming JSON format.

**Specification:** https://github.com/ndjson/ndjson-spec

**Format characteristics:**
- Each line contains ONE complete JSON object
- Lines separated by newline (`\n`)
- No outer array brackets
- Streamable and appendable
- File extension: `.ndjson`

**Configuration:**
```yaml
output:
  format: "ndjson"  # default (or "csv")
```

**Example output:**
```json
{"run_id":"1762310612183","plugin":"goertzel","window_index":0,"release_ts_ns":21194971498000,"deadline_ts_ns":21195471498000,"start_ts_ns":21194971498000,"end_ts_ns":21194971740000,"deadline_missed":0,"W":160,"H":80,"C":64,"Fs":160,"warmup":0,"repeat":1}
{"run_id":"1762310612183","plugin":"goertzel","window_index":1,"release_ts_ns":21195476495000,"deadline_ts_ns":21195976495000,"start_ts_ns":21195476495000,"end_ts_ns":21195476742000,"deadline_missed":0,"W":160,"H":80,"C":64,"Fs":160,"warmup":0,"repeat":1}
```

**Field mapping:**
All fields are identical to CSV columns (see "Common columns" above).

**Advantages:**
- Machine-readable structured format
- Streamable (no need to parse entire file)
- Compatible with tools like `jq`, Python `json.loads()`
- Can append records without rewriting file
- Standard format for log aggregation systems

**Usage with jq:**
```bash
# Pretty-print first record
head -1 results/run-*/kernel-data/*/telemetry.ndjson | jq .

# Extract all latencies
cat results/run-*/kernel-data/*/telemetry.ndjson | jq -r '.end_ts_ns - .start_ts_ns'

# Count deadline misses
cat results/run-*/kernel-data/*/telemetry.ndjson | jq -r '.deadline_missed' | grep 1 | wc -l
```

## Aggregates (per plugin per run)
- p50/p95/p99 of latency (ns)
- jitter: p95−p50, p99−p50 (ns)
- deadline_miss_rate (%)
- mean/median power (mW)
- summary table written to `results/<run-name>/analysis/SUMMARY.md`

## HTML Report

After all plugins complete execution, the harness automatically generates an interactive HTML report at `results/<run-name>/kernel-data/<kernel>/report.html` containing:

- **Summary table**: Key metrics (P50/P95/P99 latency, jitter, deadline miss rate) for all kernels
- **Per-kernel visualizations**:
  - Latency distribution histogram
  - Latency over time line plot
  - Deadline misses highlighted
- **Self-contained**: Embedded SVG plots, no external dependencies

The report is generated automatically after the plugin execution loop completes, providing a comprehensive overview of benchmark results with both summary statistics and detailed visualizations.

## Reproducibility notes
- Record git commit, build flags, CPU model, governor, turbo, and RT settings.
- Each run is isolated in its own directory for reproducibility.
- Run names provide semantic organization (e.g., `baseline-measurements`, `run-2025-11-10-001`).

## Timing Accuracy

Latency measurements use `CLOCK_MONOTONIC` for high-resolution timing:

- **macOS**: 1µs resolution (quantization to microsecond increments)
- **Linux**: Typically 1ns resolution (nanosecond precision)
- **Measurement overhead**: < 100ns (negligible)

For your midterm report, note that:
- Measurements are accurate to ±1µs on macOS (acceptable for 6-8µs latencies)
- Relative comparisons (kernel A vs B) are valid despite quantization
- Statistical aggregates (P50/P95/P99) smooth out quantization effects

To verify timing accuracy, run: `make -C tests test-clock-resolution`
